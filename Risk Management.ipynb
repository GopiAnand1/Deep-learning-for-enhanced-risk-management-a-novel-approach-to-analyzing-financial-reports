{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP8wyeOJHZEznL9r5+Tlo17"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","NLTK Fix - Run this cell FIRST to resolve punkt_tab error\n","\"\"\"\n","\n","import nltk\n","import ssl\n","\n","# Fix SSL certificate verification (sometimes needed in Colab)\n","try:\n","    _create_unverified_https_context = ssl._create_unverified_context\n","except AttributeError:\n","    pass\n","else:\n","    ssl._create_default_https_context = _create_unverified_https_context\n","\n","# List of all required NLTK packages\n","nltk_packages = [\n","    'punkt',           # Sentence tokenizer\n","    'punkt_tab',       # Updated punkt tokenizer (required for newer NLTK)\n","    'stopwords',       # Common stopwords\n","    'wordnet',         # WordNet lexical database\n","    'omw-1.4',         # Open Multilingual WordNet\n","    'averaged_perceptron_tagger',  # POS tagger\n","    'maxent_ne_chunker',  # Named entity chunker\n","    'words'            # Word corpus\n","]\n","\n","print(\"=\"*60)\n","print(\"DOWNLOADING NLTK DATA\")\n","print(\"=\"*60)\n","\n","success_count = 0\n","fail_count = 0\n","\n","for package in nltk_packages:\n","    try:\n","        print(f\"Downloading {package}...\", end=\" \")\n","        nltk.download(package, quiet=True)\n","        print(\"‚úì\")\n","        success_count += 1\n","    except Exception as e:\n","        print(f\"‚úó ({str(e)[:50]})\")\n","        fail_count += 1\n","\n","print(\"=\"*60)\n","print(f\"Summary: {success_count} successful, {fail_count} failed\")\n","print(\"=\"*60)\n","\n","# Verify installation\n","print(\"\\nVerifying NLTK installation...\")\n","try:\n","    from nltk.tokenize import word_tokenize\n","    from nltk.corpus import stopwords\n","    from nltk.stem import WordNetLemmatizer\n","\n","    test_text = \"This is a test sentence for NLTK verification.\"\n","    tokens = word_tokenize(test_text)\n","    stops = stopwords.words('english')\n","    lemmatizer = WordNetLemmatizer()\n","\n","    print(\"‚úì NLTK is working correctly!\")\n","    print(f\"  Test tokens: {tokens[:5]}\")\n","    print(f\"  Stopwords count: {len(stops)}\")\n","\n","except Exception as e:\n","    print(f\"‚úó NLTK verification failed: {e}\")\n","    print(\"\\nTry running this command manually:\")\n","    print(\"  import nltk\")\n","    print(\"  nltk.download('all')\")\n","\n","print(\"\\n‚úÖ Ready to proceed with text preprocessing!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KxXx5yiE8Euv","executionInfo":{"status":"ok","timestamp":1761120118316,"user_tz":-330,"elapsed":3250,"user":{"displayName":"Kritik Eralapally.M","userId":"15432795106853923746"}},"outputId":"23c148b5-a498-4c91-d7bf-40ea3e93857f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","DOWNLOADING NLTK DATA\n","============================================================\n","Downloading punkt... ‚úì\n","Downloading punkt_tab... ‚úì\n","Downloading stopwords... ‚úì\n","Downloading wordnet... ‚úì\n","Downloading omw-1.4... ‚úì\n","Downloading averaged_perceptron_tagger... ‚úì\n","Downloading maxent_ne_chunker... ‚úì\n","Downloading words... ‚úì\n","============================================================\n","Summary: 8 successful, 0 failed\n","============================================================\n","\n","Verifying NLTK installation...\n","‚úì NLTK is working correctly!\n","  Test tokens: ['This', 'is', 'a', 'test', 'sentence']\n","  Stopwords count: 198\n","\n","‚úÖ Ready to proceed with text preprocessing!\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3VKIMnrX22eB","executionInfo":{"status":"ok","timestamp":1761120186500,"user_tz":-330,"elapsed":63850,"user":{"displayName":"Kritik Eralapally.M","userId":"15432795106853923746"}},"outputId":"1f8a82b4-a020-4564-a1a4-02066ee69eb8"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","HYBRID FINANCIAL RISK PREDICTOR (HFRP) - COMPLETE PIPELINE\n","================================================================================\n","\n","Configuration:\n","  - Use Real Data: True\n","  - Training Epochs: 50\n","  - Batch Size: 32\n","  - Embedding Dimension: 128\n","  - LSTM Units: 64\n","  - CNN Filters: 128\n","================================================================================\n","\n","[1/15] Installing and importing libraries...\n","‚úì Libraries loaded successfully!\n","  TensorFlow version: 2.19.0\n","  GPU Available: No\n","\n","[2/15] Loading financial data...\n","  Using REAL financial data from Yahoo Finance...\n","  This will take several minutes...\n","  ‚úì AAPL   ‚úì MSFT   ‚úì GOOGL   ‚úì AMZN   ‚úì META   ‚úì NVDA   ‚úì TSLA   ‚úì JPM   ‚úì BAC   ‚úì WFC   ‚úì V   ‚úì MA   ‚úì JNJ   ‚úì PFE   ‚úì UNH   ‚úì WMT   ‚úì HD   ‚úì KO   ‚úì PEP   ‚úì XOM   ‚úì CVX   ‚úì INTC   ‚úì CSCO   ‚úì ORCL   ‚úì CRM   ‚úì ADBE   ‚úì NFLX   ‚úì PYPL   ‚úì GS   ‚úì MS   ‚úì C   ‚úì GE   ‚úì T   ‚úì VZ   ‚úì MRK \n","  ‚úì Real data loaded successfully!\n","\n","‚úì Dataset ready: 140 records, 35 companies\n","\n","[3/15] Creating visualizations...\n","‚úì Visualizations saved\n","\n","[4/15] Preprocessing text data...\n","‚úì Text preprocessed: vocab=70\n","\n","[5/15] Processing numerical features...\n","‚úì Features engineered and risk scores calculated\n","\n","[6/15] Performing PCA and clustering...\n","‚úì Clustering complete: 3 clusters identified\n","\n","[7/15] Preparing data for model training...\n","‚úì Train: 98, Val: 21, Test: 21\n","\n","[8/15] Building HFRP model architecture...\n","‚úì Model built: 334,408 parameters\n","\n","[9/15] Training HFRP model...\n","Epoch 1/50\n","\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 424ms/step - financial_output_loss: 13934760995632705437696.0000 - financial_output_mae: 56054173696.0000 - loss: 14460705996815820914688.0000 - risk_output_loss: 0.1263 - risk_output_mae: 0.2933 - val_financial_output_loss: 20796920517655080730624.0000 - val_financial_output_mae: 64375377920.0000 - val_loss: 20796920517655080730624.0000 - val_risk_output_loss: 0.0427 - val_risk_output_mae: 0.1762 - learning_rate: 0.0010\n","Epoch 2/50\n","\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - financial_output_loss: 13934760995632705437696.0000 - financial_output_mae: 56054173696.0000 - loss: 14460705996815820914688.0000 - risk_output_loss: 0.1045 - risk_output_mae: 0.2668 - val_financial_output_loss: 20796920517655080730624.0000 - val_financial_output_mae: 64375377920.0000 - val_loss: 20796920517655080730624.0000 - val_risk_output_loss: 0.0417 - val_risk_output_mae: 0.1745 - learning_rate: 0.0010\n","Epoch 3/50\n","\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - financial_output_loss: 13934760995632705437696.0000 - financial_output_mae: 56054173696.0000 - loss: 14460705996815820914688.0000 - risk_output_loss: 0.1089 - risk_output_mae: 0.2634 - val_financial_output_loss: 20796920517655080730624.0000 - val_financial_output_mae: 64375377920.0000 - val_loss: 20796920517655080730624.0000 - val_risk_output_loss: 0.0407 - val_risk_output_mae: 0.1722 - learning_rate: 0.0010\n","Epoch 4/50\n","\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 212ms/step - financial_output_loss: 13934760995632705437696.0000 - financial_output_mae: 56054173696.0000 - loss: 14460705996815820914688.0000 - risk_output_loss: 0.1058 - risk_output_mae: 0.2725 - val_financial_output_loss: 20796920517655080730624.0000 - val_financial_output_mae: 64375377920.0000 - val_loss: 20796920517655080730624.0000 - val_risk_output_loss: 0.0397 - val_risk_output_mae: 0.1693 - learning_rate: 0.0010\n","Epoch 5/50\n","\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step - financial_output_loss: 13934760995632705437696.0000 - financial_output_mae: 56054173696.0000 - loss: 14460705996815820914688.0000 - risk_output_loss: 0.1036 - risk_output_mae: 0.2611 - val_financial_output_loss: 20796920517655080730624.0000 - val_financial_output_mae: 64375377920.0000 - val_loss: 20796920517655080730624.0000 - val_risk_output_loss: 0.0391 - val_risk_output_mae: 0.1670 - learning_rate: 0.0010\n","Epoch 6/50\n","\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - financial_output_loss: 13934760995632705437696.0000 - financial_output_mae: 56054173696.0000 - loss: 14460705996815820914688.0000 - risk_output_loss: 0.1075 - risk_output_mae: 0.2722 - val_financial_output_loss: 20796920517655080730624.0000 - val_financial_output_mae: 64375377920.0000 - val_loss: 20796920517655080730624.0000 - val_risk_output_loss: 0.0381 - val_risk_output_mae: 0.1647 - learning_rate: 0.0010\n","Epoch 7/50\n","\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - financial_output_loss: 13934760995632705437696.0000 - financial_output_mae: 56054173696.0000 - loss: 14460705996815820914688.0000 - risk_output_loss: 0.0981 - risk_output_mae: 0.2676 - val_financial_output_loss: 20796920517655080730624.0000 - val_financial_output_mae: 64375377920.0000 - val_loss: 20796920517655080730624.0000 - val_risk_output_loss: 0.0375 - val_risk_output_mae: 0.1634 - learning_rate: 5.0000e-04\n","Epoch 8/50\n","\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - financial_output_loss: 13934760995632705437696.0000 - financial_output_mae: 56054173696.0000 - loss: 14460705996815820914688.0000 - risk_output_loss: 0.0987 - risk_output_mae: 0.2581 - val_financial_output_loss: 20796920517655080730624.0000 - val_financial_output_mae: 64375377920.0000 - val_loss: 20796920517655080730624.0000 - val_risk_output_loss: 0.0367 - val_risk_output_mae: 0.1609 - learning_rate: 5.0000e-04\n","Epoch 9/50\n","\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - financial_output_loss: 13934760995632705437696.0000 - financial_output_mae: 56054173696.0000 - loss: 14460705996815820914688.0000 - risk_output_loss: 0.1008 - risk_output_mae: 0.2574 - val_financial_output_loss: 20796920517655080730624.0000 - val_financial_output_mae: 64375377920.0000 - val_loss: 20796920517655080730624.0000 - val_risk_output_loss: 0.0366 - val_risk_output_mae: 0.1592 - learning_rate: 5.0000e-04\n","Epoch 10/50\n","\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - financial_output_loss: 13934760995632705437696.0000 - financial_output_mae: 56054173696.0000 - loss: 14460705996815820914688.0000 - risk_output_loss: 0.0880 - risk_output_mae: 0.2411 - val_financial_output_loss: 20796920517655080730624.0000 - val_financial_output_mae: 64375377920.0000 - val_loss: 20796920517655080730624.0000 - val_risk_output_loss: 0.0360 - val_risk_output_mae: 0.1559 - learning_rate: 5.0000e-04\n","Epoch 11/50\n","\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 129ms/step - financial_output_loss: 13934760995632705437696.0000 - financial_output_mae: 56054173696.0000 - loss: 14460705996815820914688.0000 - risk_output_loss: 0.0914 - risk_output_mae: 0.2560 - val_financial_output_loss: 20796920517655080730624.0000 - val_financial_output_mae: 64375377920.0000 - val_loss: 20796920517655080730624.0000 - val_risk_output_loss: 0.0358 - val_risk_output_mae: 0.1541 - learning_rate: 5.0000e-04\n","‚úì Training complete! Final loss: 13509991611878281838592.000000\n","\n","[10/15] Plotting training history...\n","‚úì Training plots saved\n","\n","[11/15] Evaluating model...\n","\n","Financial Metrics:\n","  Revenue: R¬≤=-1.2698, RMSE=1.75e+11\n","  Net Income: R¬≤=-1.0397, RMSE=3.57e+10\n","  EPS: R¬≤=-2.6617, RMSE=1.08e+01\n","\n","Risk Metrics:\n","  Credit: R¬≤=-0.2025, MAE=0.2654\n","  Market: R¬≤=-0.5531, MAE=0.0979\n","  Operational: R¬≤=-3.8152, MAE=0.1879\n","  Liquidity: R¬≤=-0.0002, MAE=0.1064\n","  Overall: R¬≤=-0.0167, MAE=0.0801\n","\n","[12/15] Analyzing risk mitigation...\n","\n","Risk Reduction:\n","  Credit: 0.4004 ‚Üí 0.5250 (-31.1%)\n","  Market: 0.3873 ‚Üí 0.4761 (-22.9%)\n","  Operational: 0.6907 ‚Üí 0.5027 (27.2%)\n","  Liquidity: 0.4985 ‚Üí 0.4982 (0.1%)\n","  Overall: 0.4893 ‚Üí 0.4788 (2.1%)\n","\n","[13/15] Creating comprehensive dashboard...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["‚úì Dashboard saved\n","\n","[14/15] Exporting model...\n","‚úì Model exported (hfrp_model.h5, scaler.pkl, tokenizer.json)\n","\n","[15/15] Generating final report...\n","‚úì Report saved (HFRP_Report.md)\n","\n","================================================================================\n","üéâ HFRP PROJECT COMPLETE! üéâ\n","================================================================================\n","\n","Generated Files:\n","  üìä distributions.png - Data visualizations\n","  üìä pca_clustering.png - Cluster analysis\n","  üìà training_history.png - Training curves\n","  üìä dashboard.png - Comprehensive dashboard\n","  ü§ñ hfrp_model.h5 - Trained model\n","  ‚öôÔ∏è  scaler.pkl - Feature scaler\n","  ‚öôÔ∏è  tokenizer.json - Text tokenizer\n","  üìÑ HFRP_Report.md - Final report\n","\n","Key Results:\n","  ‚úì Model Parameters: 334,408\n","  ‚úì Final Validation Loss: 20796920517655080730624.000000\n","  ‚úì Average R¬≤ Score: -1.6571\n","  ‚úì Average Risk Reduction: -4.93%\n","================================================================================\n","\n","‚úÖ All processes completed successfully!\n","You can now use the trained model for financial risk prediction.\n","================================================================================\n"]}],"source":["\"\"\"\n","HFRP MASTER SCRIPT - Complete Pipeline\n","Run this single script to execute the entire HFRP project from start to finish\n","\n","This script combines all 15 parts into one comprehensive pipeline:\n","1. Setup and Installation\n","2. Data Loading (Real Financial Data from Yahoo Finance)\n","3. Data Visualization\n","4. Text Preprocessing\n","5. Numerical Data Processing\n","6. PCA and Clustering\n","7. Data Preparation for Model\n","8. Build HFRP Model\n","9. Compile and Train Model\n","10. Training Visualization\n","11. Model Evaluation\n","12. Risk Mitigation Analysis\n","13. Comprehensive Dashboard\n","14. Model Export\n","15. Final Report Generation\n","\n","Author: Based on research by Shi et al. (2025)\n","\"\"\"\n","\n","# ============================================================================\n","# SECTION 0: CONFIGURATION\n","# ============================================================================\n","\n","# Set to True to use real data from Yahoo Finance (slower but real)\n","# Set to False to use synthetic data (faster for testing)\n","USE_REAL_DATA = True\n","\n","# Training configuration\n","EPOCHS = 50\n","BATCH_SIZE = 32\n","\n","# Model configuration\n","EMBEDDING_DIM = 128\n","LSTM_UNITS = 64\n","CNN_FILTERS = 128\n","\n","print(\"=\"*80)\n","print(\"HYBRID FINANCIAL RISK PREDICTOR (HFRP) - COMPLETE PIPELINE\")\n","print(\"=\"*80)\n","print(f\"\\nConfiguration:\")\n","print(f\"  - Use Real Data: {USE_REAL_DATA}\")\n","print(f\"  - Training Epochs: {EPOCHS}\")\n","print(f\"  - Batch Size: {BATCH_SIZE}\")\n","print(f\"  - Embedding Dimension: {EMBEDDING_DIM}\")\n","print(f\"  - LSTM Units: {LSTM_UNITS}\")\n","print(f\"  - CNN Filters: {CNN_FILTERS}\")\n","print(\"=\"*80)\n","\n","# ============================================================================\n","# SECTION 1: IMPORTS AND SETUP\n","# ============================================================================\n","\n","print(\"\\n[1/15] Installing and importing libraries...\")\n","\n","# Install required packages (uncomment if needed)\n","# !pip install -q tensorflow==2.12.0 scikit-learn pandas numpy matplotlib seaborn nltk wordcloud yfinance\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","import warnings\n","import random\n","from datetime import datetime, timedelta\n","import json\n","import joblib\n","\n","warnings.filterwarnings('ignore')\n","\n","# Download NLTK data\n","for package in ['punkt', 'stopwords', 'wordnet', 'omw-1.4']:\n","    try:\n","        nltk.download(package, quiet=True)\n","    except:\n","        pass\n","\n","# Set seeds\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","random.seed(42)\n","\n","plt.style.use('seaborn-v0_8-darkgrid')\n","sns.set_palette(\"husl\")\n","\n","print(\"‚úì Libraries loaded successfully!\")\n","print(f\"  TensorFlow version: {tf.__version__}\")\n","print(f\"  GPU Available: {'Yes' if len(tf.config.list_physical_devices('GPU')) > 0 else 'No'}\")\n","\n","# ============================================================================\n","# SECTION 2: DATA LOADING\n","# ============================================================================\n","\n","print(\"\\n[2/15] Loading financial data...\")\n","\n","if USE_REAL_DATA:\n","    print(\"  Using REAL financial data from Yahoo Finance...\")\n","    print(\"  This will take several minutes...\")\n","\n","    try:\n","        import yfinance as yf\n","\n","        # Major company tickers\n","        TICKERS = [\n","            'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'NVDA', 'TSLA', 'JPM',\n","            'BAC', 'WFC', 'V', 'MA', 'JNJ', 'PFE', 'UNH', 'WMT', 'HD', 'KO',\n","            'PEP', 'XOM', 'CVX', 'INTC', 'CSCO', 'ORCL', 'CRM', 'ADBE',\n","            'NFLX', 'PYPL', 'GS', 'MS', 'C', 'GE', 'T', 'VZ', 'MRK'\n","        ]\n","\n","        all_data = []\n","        for ticker in TICKERS:\n","            try:\n","                stock = yf.Ticker(ticker)\n","                financials = stock.financials\n","                balance_sheet = stock.balance_sheet\n","                cashflow = stock.cashflow\n","                info = stock.info\n","\n","                if not financials.empty and not balance_sheet.empty:\n","                    for date in financials.columns[:4]:  # Last 4 reports\n","                        try:\n","                            revenue = financials.loc['Total Revenue', date] if 'Total Revenue' in financials.index else np.nan\n","                            net_income = financials.loc['Net Income', date] if 'Net Income' in financials.index else np.nan\n","                            operating_income = financials.loc['Operating Income', date] if 'Operating Income' in financials.index else np.nan\n","                            total_assets = balance_sheet.loc['Total Assets', date] if 'Total Assets' in balance_sheet.index else np.nan\n","                            total_liabilities = balance_sheet.loc['Total Liabilities Net Minority Interest', date] if 'Total Liabilities Net Minority Interest' in balance_sheet.index else np.nan\n","\n","                            shares = info.get('sharesOutstanding', 1)\n","                            eps = net_income / shares if shares > 0 and not np.isnan(net_income) else np.nan\n","                            cash_flow = cashflow.loc['Operating Cash Flow', date] if 'Operating Cash Flow' in cashflow.index and date in cashflow.columns else np.nan\n","\n","                            disclosure = f\"{info.get('longName', ticker)} reported financial results. The company operates in {info.get('sector', 'various')} sector. Management focuses on operational efficiency and growth.\"\n","\n","                            all_data.append({\n","                                'Company_Name': info.get('longName', ticker),\n","                                'Ticker': ticker,\n","                                'Report_Date': date,\n","                                'Revenue': float(revenue) if not np.isnan(revenue) else None,\n","                                'Net_Income': float(net_income) if not np.isnan(net_income) else None,\n","                                'EPS': float(eps) if not np.isnan(eps) else None,\n","                                'Total_Assets': float(total_assets) if not np.isnan(total_assets) else None,\n","                                'Total_Liabilities': float(total_liabilities) if not np.isnan(total_liabilities) else None,\n","                                'Operating_Income': float(operating_income) if not np.isnan(operating_income) else None,\n","                                'Cash_Flow_Operations': float(cash_flow) if not np.isnan(cash_flow) else None,\n","                                'Textual_Disclosures': disclosure\n","                            })\n","                        except:\n","                            continue\n","                print(f\"  ‚úì {ticker}\", end=\" \", flush=True)\n","            except:\n","                print(f\"  ‚úó {ticker}\", end=\" \", flush=True)\n","\n","        df = pd.DataFrame(all_data)\n","        df = df.dropna(thresh=7)\n","        numerical_cols = ['Revenue', 'Net_Income', 'EPS', 'Total_Assets', 'Total_Liabilities', 'Operating_Income', 'Cash_Flow_Operations']\n","        for col in numerical_cols:\n","            if col in df.columns:\n","                df[col] = df.groupby('Ticker')[col].fillna(method='ffill').fillna(method='bfill')\n","        df = df.dropna(subset=numerical_cols)\n","        print(\"\\n  ‚úì Real data loaded successfully!\")\n","\n","    except Exception as e:\n","        print(f\"\\n  ‚úó Error loading real data: {str(e)}\")\n","        print(\"  Falling back to synthetic data...\")\n","        USE_REAL_DATA = False\n","\n","if not USE_REAL_DATA:\n","    print(\"  Using SYNTHETIC financial data...\")\n","\n","    companies = ['Apple Inc.', 'Microsoft Corporation', 'Amazon.com Inc.', 'Alphabet Inc.',\n","                 'Tesla Inc.', 'Meta Platforms Inc.', 'NVIDIA Corporation', 'JPMorgan Chase',\n","                 'Bank of America', 'Wells Fargo', 'Visa Inc.', 'Mastercard Inc.',\n","                 'Johnson & Johnson', 'Pfizer Inc.', 'UnitedHealth Group', 'Walmart Inc.'] * 5\n","\n","    data = []\n","    start_date = datetime(2014, 1, 1)\n","\n","    for i, company in enumerate(companies):\n","        base_revenue = np.random.uniform(10e9, 500e9)\n","        base_assets = np.random.uniform(20e9, 1000e9)\n","        growth_rate = np.random.uniform(0.02, 0.15)\n","\n","        for year in range(8):\n","            report_date = start_date + timedelta(days=365 * year + i * 30)\n","            revenue = base_revenue * (1 + growth_rate) ** year * np.random.uniform(0.95, 1.05)\n","            net_income = revenue * np.random.uniform(0.05, 0.25)\n","            total_assets = base_assets * (1 + growth_rate * 0.8) ** year\n","            total_liabilities = total_assets * np.random.uniform(0.3, 0.7)\n","            operating_income = revenue * np.random.uniform(0.1, 0.3)\n","            cash_flow = net_income * np.random.uniform(0.8, 1.2)\n","            eps = net_income / np.random.uniform(1e9, 10e9)\n","\n","            disclosure = f\"{company} reported strong financial performance. The company maintains solid market position with strategic growth initiatives and effective cost management.\"\n","\n","            data.append({\n","                'Company_Name': company,\n","                'Report_Date': report_date,\n","                'Revenue': revenue,\n","                'Net_Income': net_income,\n","                'EPS': eps,\n","                'Total_Assets': total_assets,\n","                'Total_Liabilities': total_liabilities,\n","                'Operating_Income': operating_income,\n","                'Cash_Flow_Operations': cash_flow,\n","                'Textual_Disclosures': disclosure\n","            })\n","\n","    df = pd.DataFrame(data)\n","    print(\"  ‚úì Synthetic data generated successfully!\")\n","\n","print(f\"\\n‚úì Dataset ready: {len(df)} records, {df['Company_Name'].nunique()} companies\")\n","\n","# ============================================================================\n","# SECTION 3-6: DATA ANALYSIS AND PREPROCESSING\n","# ============================================================================\n","\n","print(\"\\n[3/15] Creating visualizations...\")\n","\n","# Quick visualizations\n","numerical_cols = ['Revenue', 'Net_Income', 'EPS', 'Total_Assets', 'Total_Liabilities', 'Operating_Income', 'Cash_Flow_Operations']\n","\n","fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n","axes[0,0].hist(df['Revenue']/1e9, bins=30, color='skyblue', edgecolor='black')\n","axes[0,0].set_title('Revenue Distribution')\n","axes[0,1].hist(df['Net_Income']/1e9, bins=30, color='coral', edgecolor='black')\n","axes[0,1].set_title('Net Income Distribution')\n","axes[1,0].hist(df['EPS'], bins=30, color='lightgreen', edgecolor='black')\n","axes[1,0].set_title('EPS Distribution')\n","axes[1,1].scatter(df['Total_Assets']/1e9, df['Total_Liabilities']/1e9, alpha=0.5)\n","axes[1,1].set_title('Assets vs Liabilities')\n","plt.tight_layout()\n","plt.savefig('distributions.png', dpi=150)\n","plt.close()\n","print(\"‚úì Visualizations saved\")\n","\n","print(\"\\n[4/15] Preprocessing text data...\")\n","\n","class TextPreprocessor:\n","    def __init__(self):\n","        self.lemmatizer = WordNetLemmatizer()\n","        self.stop_words = set(stopwords.words('english'))\n","        self.tokenizer = None\n","        self.max_length = 200\n","\n","    def clean_text(self, text):\n","        text = str(text).lower()\n","        text = ''.join([c for c in text if c.isalpha() or c.isspace()])\n","        tokens = word_tokenize(text)\n","        tokens = [self.lemmatizer.lemmatize(t) for t in tokens if t not in self.stop_words and len(t) > 2]\n","        return ' '.join(tokens)\n","\n","    def fit_transform(self, texts, max_words=5000):\n","        cleaned = [self.clean_text(t) for t in texts]\n","        self.tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n","        self.tokenizer.fit_on_texts(cleaned)\n","        sequences = self.tokenizer.texts_to_sequences(cleaned)\n","        padded = pad_sequences(sequences, maxlen=self.max_length, padding='post')\n","        return padded, cleaned\n","\n","preprocessor = TextPreprocessor()\n","text_sequences, cleaned_texts = preprocessor.fit_transform(df['Textual_Disclosures'].values)\n","print(f\"‚úì Text preprocessed: vocab={len(preprocessor.tokenizer.word_index)}\")\n","\n","print(\"\\n[5/15] Processing numerical features...\")\n","\n","df_numerical = df[numerical_cols].copy()\n","df_numerical['Debt_to_Asset_Ratio'] = df['Total_Liabilities'] / df['Total_Assets']\n","df_numerical['Profit_Margin'] = df['Net_Income'] / df['Revenue']\n","df_numerical['Asset_Turnover'] = df['Revenue'] / df['Total_Assets']\n","df_numerical['ROA'] = df['Net_Income'] / df['Total_Assets']\n","df_numerical['Operating_Margin'] = df['Operating_Income'] / df['Revenue']\n","\n","# Calculate risk scores\n","credit_risk = (df_numerical['Debt_to_Asset_Ratio'] - df_numerical['Debt_to_Asset_Ratio'].min()) / (df_numerical['Debt_to_Asset_Ratio'].max() - df_numerical['Debt_to_Asset_Ratio'].min())\n","market_risk = 1 - ((df_numerical['Profit_Margin'] - df_numerical['Profit_Margin'].min()) / (df_numerical['Profit_Margin'].max() - df_numerical['Profit_Margin'].min()))\n","operational_risk = 1 - ((df_numerical['ROA'] - df_numerical['ROA'].min()) / (df_numerical['ROA'].max() - df_numerical['ROA'].min()))\n","liquidity_risk = np.random.uniform(0.3, 0.8, size=len(df_numerical))\n","risk_score = 0.3*credit_risk + 0.25*market_risk + 0.25*operational_risk + 0.2*liquidity_risk\n","\n","df_numerical['Credit_Risk'] = credit_risk\n","df_numerical['Market_Risk'] = market_risk\n","df_numerical['Operational_Risk'] = operational_risk\n","df_numerical['Liquidity_Risk'] = liquidity_risk\n","df_numerical['Risk_Score'] = risk_score\n","print(\"‚úì Features engineered and risk scores calculated\")\n","\n","print(\"\\n[6/15] Performing PCA and clustering...\")\n","\n","feature_cols_cluster = ['Revenue', 'Net_Income', 'Total_Assets', 'Total_Liabilities']\n","X_cluster = df_numerical[feature_cols_cluster].copy()\n","scaler_pca = StandardScaler()\n","X_scaled = scaler_pca.fit_transform(X_cluster)\n","\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X_scaled)\n","\n","kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n","clusters = kmeans.fit_predict(X_scaled)\n","df_numerical['Cluster'] = clusters\n","df['Cluster'] = clusters\n","\n","plt.figure(figsize=(10, 8))\n","for i in range(3):\n","    mask = clusters == i\n","    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], label=f'Cluster {i}', alpha=0.6)\n","plt.title('PCA Clustering')\n","plt.legend()\n","plt.savefig('pca_clustering.png', dpi=150)\n","plt.close()\n","print(f\"‚úì Clustering complete: {3} clusters identified\")\n","\n","# ============================================================================\n","# SECTION 7: DATA PREPARATION\n","# ============================================================================\n","\n","print(\"\\n[7/15] Preparing data for model training...\")\n","\n","feature_cols = ['Revenue', 'Net_Income', 'EPS', 'Total_Assets', 'Total_Liabilities',\n","                'Operating_Income', 'Cash_Flow_Operations', 'Debt_to_Asset_Ratio',\n","                'Profit_Margin', 'Asset_Turnover', 'ROA', 'Operating_Margin']\n","\n","X_numerical = df_numerical[feature_cols].values\n","scaler = StandardScaler()\n","X_numerical_scaled = scaler.fit_transform(X_numerical)\n","\n","y_financial = np.column_stack([df['Revenue'].values, df['Net_Income'].values, df['EPS'].values])\n","y_risk = np.column_stack([df_numerical['Credit_Risk'].values, df_numerical['Market_Risk'].values,\n","                          df_numerical['Operational_Risk'].values, df_numerical['Liquidity_Risk'].values,\n","                          df_numerical['Risk_Score'].values])\n","\n","indices = np.arange(len(X_numerical_scaled))\n","train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=42)\n","val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n","\n","X_num_train, X_num_val, X_num_test = X_numerical_scaled[train_idx], X_numerical_scaled[val_idx], X_numerical_scaled[test_idx]\n","X_text_train, X_text_val, X_text_test = text_sequences[train_idx], text_sequences[val_idx], text_sequences[test_idx]\n","y_financial_train, y_financial_val, y_financial_test = y_financial[train_idx], y_financial[val_idx], y_financial[test_idx]\n","y_risk_train, y_risk_val, y_risk_test = y_risk[train_idx], y_risk[val_idx], y_risk[test_idx]\n","\n","print(f\"‚úì Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n","\n","# ============================================================================\n","# SECTION 8-9: BUILD AND TRAIN MODEL\n","# ============================================================================\n","\n","print(\"\\n[8/15] Building HFRP model architecture...\")\n","\n","vocab_size = len(preprocessor.tokenizer.word_index) + 1\n","text_max_length = preprocessor.max_length\n","num_features = len(feature_cols)\n","\n","# Text branch (CNN)\n","text_input = Input(shape=(text_max_length,), name='text_input')\n","text_embedded = Embedding(vocab_size, EMBEDDING_DIM, input_length=text_max_length)(text_input)\n","conv1 = Conv1D(CNN_FILTERS, 3, activation='relu', padding='same')(text_embedded)\n","conv1 = BatchNormalization()(conv1)\n","pool1 = MaxPooling1D(2)(conv1)\n","drop1 = Dropout(0.3)(pool1)\n","conv2 = Conv1D(CNN_FILTERS*2, 3, activation='relu', padding='same')(drop1)\n","conv2 = BatchNormalization()(conv2)\n","pool2 = MaxPooling1D(2)(conv2)\n","text_features = GlobalMaxPooling1D()(pool2)\n","text_dense = Dense(128, activation='relu')(text_features)\n","text_output = Dropout(0.3)(text_dense)\n","\n","# Numerical branch (LSTM)\n","num_input = Input(shape=(num_features,), name='numerical_input')\n","num_reshaped = tf.keras.layers.Reshape((num_features, 1))(num_input)\n","lstm1 = LSTM(LSTM_UNITS, return_sequences=True)(num_reshaped)\n","lstm1 = BatchNormalization()(lstm1)\n","lstm1 = Dropout(0.3)(lstm1)\n","lstm2 = LSTM(LSTM_UNITS)(lstm1)\n","lstm2 = BatchNormalization()(lstm2)\n","num_output = Dropout(0.3)(lstm2)\n","\n","# Combined\n","combined = Concatenate()([text_output, num_output])\n","dense1 = Dense(256, activation='relu')(combined)\n","dense1 = BatchNormalization()(dense1)\n","dense1 = Dropout(0.4)(dense1)\n","dense2 = Dense(128, activation='relu')(dense1)\n","dense2 = BatchNormalization()(dense2)\n","dense2 = Dropout(0.4)(dense2)\n","dense3 = Dense(64, activation='relu')(dense2)\n","dense3 = Dropout(0.3)(dense3)\n","\n","financial_output = Dense(3, activation='linear', name='financial_output')(dense3)\n","risk_output = Dense(5, activation='sigmoid', name='risk_output')(dense3)\n","\n","model = Model(inputs=[text_input, num_input], outputs=[financial_output, risk_output], name='HFRP')\n","print(f\"‚úì Model built: {model.count_params():,} parameters\")\n","\n","print(\"\\n[9/15] Training HFRP model...\")\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(0.001),\n","    loss={'financial_output': 'mse', 'risk_output': 'mse'},\n","    loss_weights={'financial_output': 1.0, 'risk_output': 1.0},\n","    metrics={'financial_output': ['mae'], 'risk_output': ['mae']}\n",")\n","\n","callbacks = [\n","    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n","    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n","]\n","\n","history = model.fit(\n","    [X_text_train, X_num_train],\n","    [y_financial_train, y_risk_train],\n","    validation_data=([X_text_val, X_num_val], [y_financial_val, y_risk_val]),\n","    epochs=EPOCHS,\n","    batch_size=BATCH_SIZE,\n","    callbacks=callbacks,\n","    verbose=1\n",")\n","\n","print(f\"‚úì Training complete! Final loss: {history.history['loss'][-1]:.6f}\")\n","\n","# ============================================================================\n","# SECTION 10-12: EVALUATION\n","# ============================================================================\n","\n","print(\"\\n[10/15] Plotting training history...\")\n","\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], label='Train')\n","plt.plot(history.history['val_loss'], label='Val')\n","plt.title('Training Loss')\n","plt.legend()\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['financial_output_mae'], label='Financial MAE')\n","plt.plot(history.history['risk_output_mae'], label='Risk MAE')\n","plt.title('MAE Progress')\n","plt.legend()\n","plt.tight_layout()\n","plt.savefig('training_history.png', dpi=150)\n","plt.close()\n","print(\"‚úì Training plots saved\")\n","\n","print(\"\\n[11/15] Evaluating model...\")\n","\n","predictions = model.predict([X_text_test, X_num_test], verbose=0)\n","financial_pred, risk_pred = predictions[0], predictions[1]\n","\n","def calc_metrics(y_true, y_pred):\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(y_true, y_pred)\n","    r2 = r2_score(y_true, y_pred)\n","    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2}\n","\n","print(\"\\nFinancial Metrics:\")\n","for i, label in enumerate(['Revenue', 'Net Income', 'EPS']):\n","    m = calc_metrics(y_financial_test[:, i], financial_pred[:, i])\n","    print(f\"  {label}: R¬≤={m['R2']:.4f}, RMSE={m['RMSE']:.2e}\")\n","\n","print(\"\\nRisk Metrics:\")\n","for i, label in enumerate(['Credit', 'Market', 'Operational', 'Liquidity', 'Overall']):\n","    m = calc_metrics(y_risk_test[:, i], risk_pred[:, i])\n","    print(f\"  {label}: R¬≤={m['R2']:.4f}, MAE={m['MAE']:.4f}\")\n","\n","print(\"\\n[12/15] Analyzing risk mitigation...\")\n","\n","risk_before = y_risk_test.mean(axis=0)\n","risk_after = risk_pred.mean(axis=0)\n","risk_reduction = ((risk_before - risk_after) / risk_before) * 100\n","\n","print(\"\\nRisk Reduction:\")\n","risk_labels = ['Credit', 'Market', 'Operational', 'Liquidity', 'Overall']\n","for i, label in enumerate(risk_labels):\n","    print(f\"  {label}: {risk_before[i]:.4f} ‚Üí {risk_after[i]:.4f} ({risk_reduction[i]:.1f}%)\")\n","\n","# ============================================================================\n","# SECTION 13-15: EXPORT AND REPORTING\n","# ============================================================================\n","\n","print(\"\\n[13/15] Creating comprehensive dashboard...\")\n","\n","fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n","axes[0,0].plot(history.history['loss'])\n","axes[0,0].set_title('Training Loss')\n","axes[0,1].bar(risk_labels, risk_reduction)\n","axes[0,1].set_title('Risk Reduction %')\n","axes[1,0].scatter(y_financial_test[:,0], financial_pred[:,0], alpha=0.5)\n","axes[1,0].plot([y_financial_test[:,0].min(), y_financial_test[:,0].max()],\n","               [y_financial_test[:,0].min(), y_financial_test[:,0].max()], 'r--')\n","axes[1,0].set_title('Revenue: Actual vs Predicted')\n","x = np.arange(len(risk_labels))\n","axes[1,1].bar(x-0.2, risk_before, 0.4, label='Before')\n","axes[1,1].bar(x+0.2, risk_after, 0.4, label='After')\n","axes[1,1].set_xticks(x)\n","axes[1,1].set_xticklabels(risk_labels, rotation=45)\n","axes[1,1].set_title('Risk Comparison')\n","axes[1,1].legend()\n","plt.tight_layout()\n","plt.savefig('dashboard.png', dpi=150)\n","plt.close()\n","print(\"‚úì Dashboard saved\")\n","\n","print(\"\\n[14/15] Exporting model...\")\n","\n","model.save('hfrp_model.h5')\n","joblib.dump(scaler, 'scaler.pkl')\n","with open('tokenizer.json', 'w') as f:\n","    json.dump(preprocessor.tokenizer.to_json(), f)\n","print(\"‚úì Model exported (hfrp_model.h5, scaler.pkl, tokenizer.json)\")\n","\n","print(\"\\n[15/15] Generating final report...\")\n","\n","report = f\"\"\"# HFRP Final Report\n","\n","## Summary\n","- Dataset: {len(df)} records, {df['Company_Name'].nunique()} companies\n","- Training samples: {len(train_idx)}\n","- Final loss: {history.history['loss'][-1]:.6f}\n","\n","## Performance\n","- Avg Financial R¬≤: {np.mean([calc_metrics(y_financial_test[:,i], financial_pred[:,i])['R2'] for i in range(3)]):.4f}\n","- Avg Risk R¬≤: {np.mean([calc_metrics(y_risk_test[:,i], risk_pred[:,i])['R2'] for i in range(5)]):.4f}\n","- Avg Risk Reduction: {risk_reduction.mean():.2f}%\n","\n","Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n","\"\"\"\n","\n","with open('HFRP_Report.md', 'w') as f:\n","    f.write(report)\n","print(\"‚úì Report saved (HFRP_Report.md)\")\n","\n","# ============================================================================\n","# FINAL SUMMARY\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"üéâ HFRP PROJECT COMPLETE! üéâ\")\n","print(\"=\"*80)\n","print(\"\\nGenerated Files:\")\n","print(\"  üìä distributions.png - Data visualizations\")\n","print(\"  üìä pca_clustering.png - Cluster analysis\")\n","print(\"  üìà training_history.png - Training curves\")\n","print(\"  üìä dashboard.png - Comprehensive dashboard\")\n","print(\"  ü§ñ hfrp_model.h5 - Trained model\")\n","print(\"  ‚öôÔ∏è  scaler.pkl - Feature scaler\")\n","print(\"  ‚öôÔ∏è  tokenizer.json - Text tokenizer\")\n","print(\"  üìÑ HFRP_Report.md - Final report\")\n","print(\"\\nKey Results:\")\n","print(f\"  ‚úì Model Parameters: {model.count_params():,}\")\n","print(f\"  ‚úì Final Validation Loss: {history.history['val_loss'][-1]:.6f}\")\n","print(f\"  ‚úì Average R¬≤ Score: {np.mean([calc_metrics(y_financial_test[:,i], financial_pred[:,i])['R2'] for i in range(3)]):.4f}\")\n","print(f\"  ‚úì Average Risk Reduction: {risk_reduction.mean():.2f}%\")\n","print(\"=\"*80)\n","print(\"\\n‚úÖ All processes completed successfully!\")\n","print(\"You can now use the trained model for financial risk prediction.\")\n","print(\"=\"*80)"]}]}